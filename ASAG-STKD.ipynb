{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "packed-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, string, spacy, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from scipy import spatial\n",
    "\n",
    "import en_core_web_sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "operational-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Spacy\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de43b558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n"
     ]
    }
   ],
   "source": [
    "# Load InferSent model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from models import InferSent\n",
    "\n",
    "V = 2\n",
    "MODEL_PATH = 'models/InferSent/encoder/infersent%s.pkl' % V\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "W2V_PATH = 'models/InferSent/fastText/crawl-300d-2M.vec'\n",
    "infersent.set_w2v_path(W2V_PATH)\n",
    "\n",
    "infersent.build_vocab_k_words(K=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29c325f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT\n",
    "\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85802856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rougeL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b361c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "w2v_size = 300\n",
    "infersent_size = 4096\n",
    "bert_size = 768\n",
    "deconf_size = 300\n",
    "\n",
    "max_emb_size = infersent_size\n",
    "\n",
    "lemma_pos_tags = ['ADJ', 'ADV', 'NOUN', 'VERB', 'NUM', 'PROPN']\n",
    "\n",
    "def get_lemmas(text):\n",
    "    if text == '':\n",
    "        return []\n",
    "    return list(filter(None, [t.split('\\t')[2] for t in text.split('\\n') if t.split('\\t')[1] in lemma_pos_tags]))\n",
    "\n",
    "def get_emb_from_string(text):\n",
    "    return [float(c) for c in text.split()[1:]]\n",
    "\n",
    "def get_mean_emb(embeddings):\n",
    "    if len(embeddings)==0:\n",
    "        return np.zeros(max_emb_size)\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "def process(text):  \n",
    "    processed = nlp(text)\n",
    "    sentence = list()\n",
    "        \n",
    "    for word in processed:\n",
    "        item = list()\n",
    "        item.append(str(word))\n",
    "        item.append(word.pos_)\n",
    "        item.append(word.lemma_)        \n",
    "        sentence.append('\\t'.join(item))\n",
    "    return '\\n'.join(sentence)\n",
    "\n",
    "def get_w2v(text):\n",
    "    lemmatized = get_lemmas(text) \n",
    "    embeddings = list()\n",
    "    for lemma in lemmatized:\n",
    "        map_emb = w2v_mapping.get(lemma)\n",
    "        if map_emb:\n",
    "            emb = get_emb_from_string(w2v_flines[map_emb])\n",
    "            embeddings.append(emb)\n",
    "    return get_mean_emb(embeddings)\n",
    "\n",
    "def get_infersent(text):\n",
    "    lemmatized = get_lemmas(text) \n",
    "    emb = infersent.encode([' '.join(lemmatized)], bsize=128, tokenize=False)[0]\n",
    "    if len(emb) == 0:\n",
    "        emb = np.zeros(max_emb_size)\n",
    "    return emb\n",
    "\n",
    "def get_bert(text):\n",
    "    lemmatized = get_lemmas(text) \n",
    "    inputs = tokenizer(' '.join(lemmatized), return_tensors=\"pt\")\n",
    "    outputs = bert(**inputs)\n",
    "    lhs = outputs.last_hidden_state\n",
    "    attention = inputs['attention_mask'].reshape((lhs.size()[0], lhs.size()[1], -1)).expand(-1, -1, 768)\n",
    "    embeddings = torch.mul(lhs, attention)\n",
    "    denominator = (embeddings != 0).sum(dim=1)\n",
    "    summation = torch.sum(embeddings, dim=1)\n",
    "    mean_embedding = torch.div(summation, denominator)[0].detach().numpy()\n",
    "    if len(mean_embedding) == 0:\n",
    "        mean_embedding = np.zeros(max_emb_size)\n",
    "    return mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0da643a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity functions\n",
    "\n",
    "def cos(a, b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "def jaccard_sim(l1, l2): \n",
    "    if len(l2) == 0:\n",
    "        return 0\n",
    "    a = set(l1) \n",
    "    b = set(l2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def word_overlap(l1, l2):\n",
    "    if len(l2) == 0 or len(l1) == 0:\n",
    "        return 0\n",
    "    a = set(l1) \n",
    "    b = set(l2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / min(len(a),len(b))\n",
    "\n",
    "def get_rouges(t1, t2):\n",
    "    return rouge.score(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e0a0146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe utility functions\n",
    "\n",
    "def process_df(df):\n",
    "    print('- text processing')\n",
    "    df['processed'] = df['text'].progress_apply(lambda x: process(x))\n",
    "\n",
    "def get_w2v4df(df):\n",
    "    print('- computing Word2Vec embeddings')\n",
    "    df['w2v'] = df['processed'].progress_apply(lambda x: get_w2v(x))  \n",
    "    \n",
    "def get_infersent4df(df):\n",
    "    print('- computing InferSent embeddings')\n",
    "    df['infersent'] = df['processed'].progress_apply(lambda x: get_infersent(x)) \n",
    "    \n",
    "def get_bert4df(df):\n",
    "    print('- computing BERT embeddings')\n",
    "    df['bert'] = df['processed'].progress_apply(lambda x: get_bert(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e64e4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature generation and grade prediction functions\n",
    "\n",
    "def generate_embedding_feats(ans_df, ref_df, que_df, emb):\n",
    "    \n",
    "    col_suf = '_' + emb\n",
    "    zeros = np.zeros(len(ans_df.index))\n",
    "    emb_size = globals()[emb + '_size']\n",
    "    \n",
    "    ans_df['Diff' + col_suf] = ''\n",
    "    \n",
    "    for i in ans_df.index:\n",
    "        ans_row = ans_df.loc[i]\n",
    "        ref_row = ref_df[ref_df.key==ans_row.key]\n",
    "        que_row = que_df[que_df.key==ans_row.key]\n",
    "        \n",
    "        ans_emb = ans_row[emb]\n",
    "        ref_emb = ref_row[emb].values\n",
    "            \n",
    "        if len(ans_emb) > 0 and len(ref_emb) > 0:\n",
    "            \n",
    "            ans_emb = ans_emb[:emb_size]\n",
    "            ref_emb = ref_emb[0][:emb_size]\n",
    "            \n",
    "            if np.all(ans_emb==0):\n",
    "                ans_df.at[i, 'Diff' + col_suf] = np.zeros(emb_size)\n",
    "            else:\n",
    "                diff = ans_emb - ref_emb\n",
    "\n",
    "                ans_df.at[i, 'Diff'  + col_suf] = diff\n",
    "                \n",
    "def generate_text_feats(ans_df, ref_df, que_df):\n",
    "    \n",
    "    zeros = np.zeros(len(ans_df.index))\n",
    "    \n",
    "    ans_df['Jaccard'] = zeros\n",
    "    ans_df['Overlap']  = zeros\n",
    "    ans_df['NumNouns'] = zeros\n",
    "    ans_df['NumVerbs'] = zeros\n",
    "    ans_df['Rouge1'] = zeros\n",
    "    ans_df['RougeL'] = zeros\n",
    "        \n",
    "    for i in tqdm(ans_df.index):\n",
    "        ans_row = ans_df.loc[i]\n",
    "        ref_row = ref_df[ref_df.key==ans_row.key]\n",
    "        que_row = que_df[que_df.key==ans_row.key]\n",
    "\n",
    "        if ans_row['processed'] == '' or ref_row['processed'].values[0] == '':\n",
    "            continue\n",
    "            \n",
    "        ans_lemmas = [t.split('\\t')[2] for t in ans_row['processed'].split('\\n') \n",
    "                      if t.split('\\t')[1] in lemma_pos_tags]\n",
    "        ref_lemmas = [t.split('\\t')[2] for t in ref_row['processed'].values[0].split('\\n') \n",
    "                      if t.split('\\t')[1] in lemma_pos_tags]\n",
    "        que_lemmas = [t.split('\\t')[2] for t in que_row['processed'].values[0].split('\\n') \n",
    "                      if t.split('\\t')[1] in lemma_pos_tags]\n",
    "        \n",
    "         # question demoting\n",
    "        for q in que_lemmas:\n",
    "            if q in ans_lemmas:\n",
    "                ans_lemmas.remove(q)\n",
    "                \n",
    "        if len(ans_lemmas) == 0 or len(ref_lemmas) == 0:\n",
    "            continue\n",
    "         \n",
    "        ans_df.at[i, 'Overlap']  = word_overlap(ans_lemmas, ref_lemmas)\n",
    "        ans_df.at[i, 'Jaccard'] = jaccard_sim(ans_lemmas, ref_lemmas)\n",
    "\n",
    "        rouges = get_rouges(' '.join(ans_lemmas), ' '.join(ref_lemmas))\n",
    "        ans_df.at[i, 'Rouge1'] = rouges['rouge1'].fmeasure\n",
    "        ans_df.at[i, 'RougeL'] = rouges['rougeL'].fmeasure\n",
    "            \n",
    "        num_nouns_ans = sum([1 for t in ans_row['processed'].split('\\n') \n",
    "                             if t.split('\\t')[1] in ['NOUN', 'PROPN']])\n",
    "        num_verbs_ans = sum([1 for t in ans_row['processed'].split('\\n') \n",
    "                             if t.split('\\t')[1] == 'VERB'])\n",
    "        ans_df.at[i, 'NumNouns'] = num_nouns_ans\n",
    "        ans_df.at[i, 'NumVerbs'] = num_verbs_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "632f27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'SVR': make_pipeline(StandardScaler(), SVR())\n",
    "}\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    scores = cross_val_predict(model, list(X), list(y), cv=5)\n",
    "    return scores\n",
    "\n",
    "def stacked_regr(df, cols, emb, mode=None):\n",
    "    suf = ''\n",
    "    if mode:\n",
    "        suf += '_' + mode\n",
    "    X = df[cols + ['Diff_' + emb + suf]]\n",
    "    y = df.score\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        scores = evaluate_model(model, X_train['Diff_'+ emb + suf], y_train)\n",
    "        X_train[name] = scores\n",
    "        model.fit(list(X_train['Diff_'+ emb + suf]), y_train)\n",
    "        scores = model.predict(list(X_test['Diff_'+ emb + suf]))\n",
    "        X_test[name] = scores\n",
    "        cols+=[name]\n",
    "\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), ('model', Lasso())])\n",
    "    \n",
    "    # Search for optimal Lasso hyperparams\n",
    "    search = GridSearchCV(pipeline,\n",
    "                      {'model__alpha':np.arange(0.1,10,0.1)},\n",
    "                      cv = 5, scoring=\"neg_mean_squared_error\")\n",
    "    \n",
    "    search.fit(list(X_train[cols].values), y_train)\n",
    "    \n",
    "    coefficients = search.best_estimator_.named_steps['model'].coef_\n",
    "    importance = np.abs(coefficients)\n",
    "    \n",
    "    # Select important features\n",
    "    new_cols = X_train[cols].columns[importance>0]\n",
    "    \n",
    "    print('Regression results - {}:'.format(emb))\n",
    "    print('- best features: {}'.format(list(new_cols)))\n",
    "    \n",
    "    regressor = make_pipeline(StandardScaler(), SVR())\n",
    "    regressor.fit(list(X_train[new_cols].values), y_train)\n",
    "    \n",
    "    y_pred = regressor.predict(list(X_test[new_cols].values))\n",
    "    r = np.corrcoef(y_pred,y_test)\n",
    "    rmse = sqrt(mean_squared_error(y_pred,y_test))\n",
    "    print('- Pearson\\'s r:', round(r[0,1]*100, 2))\n",
    "    print('- RMSE:', round(rmse*100, 2), '\\n')\n",
    "    return new_cols, pd.DataFrame(list(zip(X_test.index, y_pred, abs(y_test-y_pred))), columns=['', 'pred', 'diff']).set_index('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bronze-mexican",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing questions:\n",
      "- text processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:01<00:00, 45.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- computing Word2Vec embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:00<00:00, 953.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- computing InferSent embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:05<00:00, 14.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- computing BERT embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:06<00:00, 13.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing reference answers:\n",
      "- text processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:00<00:00, 94.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- computing Word2Vec embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:00<00:00, 655.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- computing InferSent embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:06<00:00, 12.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- computing BERT embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:07<00:00, 11.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing student answers:\n",
      "- text processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2442/2442 [00:26<00:00, 93.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- computing Word2Vec embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2442/2442 [00:04<00:00, 509.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- computing InferSent embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2442/2442 [04:17<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- computing BERT embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2442/2442 [04:48<00:00,  8.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reading answers and populating dataframes\n",
    "\n",
    "keys = os.listdir('data/ShortAnswerGrading_v2.0/data/scores')\n",
    "\n",
    "raw_stud_ans = list()\n",
    "\n",
    "for key in keys: \n",
    "    with open('data/ShortAnswerGrading_v2.0/data/scores/' + key + '/ave', 'r') as f:\n",
    "        score_lines = f.read().split('\\n')\n",
    "    with open('data/ShortAnswerGrading_v2.0/data/sent/' + key, 'r') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "        for l in range(0, len(lines)-1):\n",
    "            line = lines[l].split(' ', 1)\n",
    "            raw_stud_ans.append([line[0], line[1], float(score_lines[l])])\n",
    "            \n",
    "raw_ref_ans = list()\n",
    "\n",
    "with open('data/ShortAnswerGrading_v2.0/data/sent/answers', 'r') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    for l in range(0, len(lines)-1):\n",
    "        line = lines[l].split(' ', 1)\n",
    "        raw_ref_ans.append((line[0], line[1]))\n",
    "        \n",
    "raw_que = list()\n",
    "\n",
    "with open('data/ShortAnswerGrading_v2.0/data/sent/questions', 'r') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    for l in range(0, len(lines)-1):\n",
    "        line = lines[l].split(' ', 1)\n",
    "        raw_que.append((line[0], line[1]))     \n",
    "\n",
    "# Read Word2Vec\n",
    "w2v_path = 'models/GoogleNews-vectors-negative300.txt'\n",
    "with open(w2v_path, 'r', encoding='utf-8') as f:\n",
    "    f.readline()\n",
    "    w2v_flines = f.readlines()\n",
    "w2v_mapping = dict()\n",
    "for i in range(0, len(w2v_flines)):\n",
    "    w2v_mapping[w2v_flines[i].split()[0]] = i  \n",
    "\n",
    "print('Processing questions:')\n",
    "que_df = pd.DataFrame(raw_que, columns=['key', 'text'])\n",
    "process_df(que_df)\n",
    "get_w2v4df(que_df)\n",
    "get_infersent4df(que_df)\n",
    "get_bert4df(que_df)\n",
    "\n",
    "print('Processing reference answers:')\n",
    "ref_ans_df = pd.DataFrame(raw_ref_ans, columns=['key', 'text'])\n",
    "process_df(ref_ans_df)\n",
    "get_w2v4df(ref_ans_df)\n",
    "get_infersent4df(ref_ans_df)\n",
    "get_bert4df(ref_ans_df)\n",
    "\n",
    "print('Processing student answers:')\n",
    "stud_ans_df = pd.DataFrame(raw_stud_ans, columns=['key', 'text', 'score'])\n",
    "process_df(stud_ans_df)\n",
    "get_w2v4df(stud_ans_df)\n",
    "get_infersent4df(stud_ans_df)\n",
    "get_bert4df(stud_ans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ad6f10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2442/2442 [00:03<00:00, 657.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generating features for UNT\n",
    "\n",
    "generate_text_feats(stud_ans_df, ref_ans_df, que_df)\n",
    "generate_embedding_feats(stud_ans_df, ref_ans_df, que_df, 'w2v')\n",
    "generate_embedding_feats(stud_ans_df, ref_ans_df, que_df, 'infersent')\n",
    "generate_embedding_feats(stud_ans_df, ref_ans_df, que_df, 'bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "382753d4",
   "metadata": {},
   "outputs": [],
   "source": [
    " cols = ['Jaccard','Overlap', 'NumNouns', 'NumVerbs', 'Rouge1', 'RougeL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d8aab7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-STKD-\n",
      "\n",
      "Regression results - w2v:\n",
      "- best features: ['Overlap', 'Rouge1', 'SVR']\n",
      "- Pearson's r: 67.64\n",
      "- RMSE: 76.28 \n",
      "\n",
      "Regression results - infersent:\n",
      "- best features: ['Overlap', 'SVR']\n",
      "- Pearson's r: 66.74\n",
      "- RMSE: 77.15 \n",
      "\n",
      "Regression results - bert:\n",
      "- best features: ['Overlap', 'SVR']\n",
      "- Pearson's r: 70.36\n",
      "- RMSE: 73.35 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STKD (stacked) model\n",
    "\n",
    "print('-STKD-\\n')\n",
    "\n",
    "# Stacked regression w2v\n",
    "feats_w2v, res_df_w2v = stacked_regr(stud_ans_df, cols.copy(), emb='w2v')\n",
    "\n",
    "# Stacked regression InferSent\n",
    "feats_infersent, res_df_infersent = stacked_regr(stud_ans_df, cols.copy(), emb='infersent')\n",
    "\n",
    "# Stacked regression BERT\n",
    "feats_bert, res_df_bert = stacked_regr(stud_ans_df, cols.copy(), emb='bert')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
